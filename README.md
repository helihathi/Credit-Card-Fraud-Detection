# ğŸŒğŸ” Credit Card Fraud Detection â€” Real-Time ML Pipeline (Docker-only)
### ğŸš€ End-to-End Streaming System using Kafka, Spark, FastAPI, XGBoost, Streamlit & Docker

> âš ï¸ **Important:** This repository runs *only via Docker / Docker Compose*. Manual execution (`python app.py`, `streamlit run`, etc.) is **not supported** for reproducing the end-to-end pipeline. Everything below assumes Docker is available on your machine.

---

## ğŸ“Œ Overview

This project provides a **real-time credit card fraud detection pipeline** using Kafka for ingestion, Spark Structured Streaming for processing, FastAPI for inference, Streamlit for visualization, and SQLite for persistenceâ€”all fully containerized via Docker.

---

## âœ… Quick highlights

- Docker-first: the full stack runs only inside containers.  
- Reproducible: clone â†’ `docker-compose up --build -d` â†’ open dashboard.  
- Model files included: `model/xgboost_caliberated.joblib`, `model/standard_scaler.joblib` (13 MB).

---

## ğŸ“ Folder Structure & Explanation

```
credit-card-fraud-detection/
â”‚
â”œâ”€â”€ data/                                # Dataset + runtime databases
â”‚
â”œâ”€â”€ fastapi/                             # Fraud inference microservice
â”‚   â”œâ”€â”€ app.py                           # FastAPI endpoints
â”‚   â”œâ”€â”€ alert_email.py                   # Email alert handler
â”‚   â”œâ”€â”€ Dockerfile                       # FastAPI container build file
â”‚   â””â”€â”€ requirements.txt                 # FastAPI dependencies
â”‚
â”œâ”€â”€ fraud-dashboard/                     # Streamlit dashboard module
â”‚   â””â”€â”€ realtime_dashboard.py            # Main dashboard file
â”‚
â”œâ”€â”€ model/                               # ML model artifacts
â”‚   â”œâ”€â”€ xgboost_caliberated.joblib       # Trained classifier
â”‚   â””â”€â”€ standard_scaler.joblib           # Preprocessing scaler
â”‚
â”œâ”€â”€ notebook/                            # Training notebooks
â”‚   â””â”€â”€ Credit_Card_Fraud_detection.ipynb
â”‚
â”œâ”€â”€ producer/                            # Kafka transaction generator
â”‚   â”œâ”€â”€ producer.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements-producer.txt
â”‚
â”œâ”€â”€ spark-consumer/                      # Spark streaming consumer
â”‚   â”œâ”€â”€ consumer.py
â”‚   â”œâ”€â”€ alert_email.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ consumer-requirements.txt
â”‚
â”œâ”€â”€ .dockerignore                        # Excludes unnecessary files from Docker context
â”œâ”€â”€ .gitignore                           # Files ignored by Git
â”œâ”€â”€ .env                                 # Environment variables (not included in repo)
â”œâ”€â”€ create_db.py                         # Database schema initializer
â”œâ”€â”€ migrate_db.py                        # DB alteration scripts
â”œâ”€â”€ etl.py                               # Loads dataset into DB
â”œâ”€â”€ docker-compose.yml                   # Orchestrates entire pipeline
â”œâ”€â”€ Dockerfile                           # Dashboard Dockerfile
â”œâ”€â”€ Dockerfile.base                      # Cached heavy dependencies layer
â”œâ”€â”€ realtime_dashboard.py                # Legacy dashboard entry
â”œâ”€â”€ fraud_detection.db                   # Local DB created manually
â””â”€â”€ requirements.txt                     # Dashboard requirements
```

---


## ğŸ”’ `.env` Template (Use `.env.example` in repo)
Add `.env` to `.gitignore`

---

## ğŸš€ Step-by-step Docker execution (recommended)

### A â€” Full stack with docker-compose (one command)

1. Build and start everything:

```bash
docker-compose up --build -d
```

2. Check the containers are running:

```bash
docker-compose ps
```

3. View aggregated logs (follow):

```bash
docker-compose logs -f
```

4. To stop and remove containers (keep volumes):

```bash
docker-compose down
```

5. To stop and remove including volumes (WARNING: this deletes DB data):

```bash
docker-compose down -v
```

---

## ğŸ” How to execute the container
## Start Zookeeper + Kafka via docker-compose, then start the stack

First pull the `zookeeper` and `kafka` images from docker using
```bash
# Pull Zookeeper Image from docker
docker pull bitnamilegacy/zookeeper:3.6.2

# Pull Kafka Image from docker
docker pull bitnamilegacy/kafka:3.4.0
```

If your `docker-compose.yml` already defines `zookeeper` and `kafka` services, you can bring them up first and wait, then start others:

```bash
# start only zookeeper first
docker-compose up -d zookeeper

# start only kafka (after 30 seconds of starting zookeeper)
docker compose up -d kafka

# then start the rest (after starting kafka)
docker-compose up -d producer spark-consumer fastapi fraud-dashboard
```

This is advisable just to ensure Kafka is ready before consumers connect.

---

## ğŸ›  Build individual containers (if you change code)

Rebuild specific services after code changes:

```bash
# rebuild only fastapi and spark-consumer
docker-compose build fastapi spark-consumer

# bring them up
docker-compose up -d fastapi spark-consumer
```

---

- List Docker containers:

```bash
docker ps -a
```

- Remove a single container:

```bash
docker rm -f container_name
```

---

## ğŸ§¾ Database notes

- The SQLite DB files are created/managed at runtime in `data/` (or wherever your `docker-compose` mounts the volume). Do **not** commit `.db`, `.db-shm`, or `.db-wal` files to GitHub. Use `create_db.py` + `etl.py` to recreate schema and seed data if needed.

---

## âœ… Health checks & quick verification

1. `docker-compose ps` â†’ ensure services show `Up`.  
2. `docker-compose logs -f spark-consumer` â†’ watch streaming predictions.  
3. Open dashboard: `http://localhost:8501` (or `FASTAPI_PORT` / `DASHBOARD_PORT` in `.env`).  
4. Test FastAPI health endpoint:

```bash
curl http://localhost:8000/health
```

5. Post a sample transaction to FastAPI `/predict` to confirm model loads and responds.

---

## ğŸ” Model updates (Docker workflow)

If you retrain the model locally and create new joblib files:

1. Replace the files in `model/` locally.  
2. Rebuild images that bundle the model (typically `spark-consumer` and `fastapi`):

```bash
docker-compose build spark-consumer fastapi
docker-compose up -d spark-consumer fastapi
```

Alternatively, mount `./model` as a volume into the containers so you can swap models without rebuilding.

---

## âš ï¸ Troubleshooting tips

- If a consumer cannot connect to Kafka, ensure Kafka advertises the listener reachable by the container (check `KAFKA_ADVERTISED_LISTENERS`).  
- If ports are in use (2181/9092/8000/8501), change them in `.env` and `docker-compose.yml`.  
- If images fail to build due to missing model files, ensure `model/` contains the `.joblib` files or configure auto-download in the Dockerfile.

---

## ğŸ–¼ Screenshots

```markdown
<p align="center">
  <img src="docs/dashboard.png" width="700" alt="Dashboard screenshot"/>
</p>
```

---

## ğŸ”— Dataset (source & download)

This project uses the popular **Kaggle Credit Card Fraud Detection** dataset:

- **Kaggle dataset page:** https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
---
## ğŸ“œ License

This repository uses the **MIT License** (see `LICENSE`).

---
